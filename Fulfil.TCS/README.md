# Repack service

The repack service runs only in PLM and responds to FC with requests for whether a bag is empty in a particular Bay.

# CI / CD

Open a PR to auto-build a branch in prod GCR of the same name.  Merging your PR will also rebuild the main branch which is used by PLM.


TODO: Better CD, currently a manual restart is required to pull the latest main image.

# Logging / KPI Graphs

Currently the [Grafana dashboard](https://grafana.fulfil-api.com/d/f8533408-b7a4-4ba7-a087-e95be8d905e3/repack-service?orgId=1&var-ds=P59E27672EF15730F&from=now-3h&to=now) and [raw log search](https://grafana.fulfil-api.com/explore?panes=%7B%22MYp%22:%7B%22datasource%22:%22P59E27672EF15730F%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22%7Bnamespace%3D%5C%22fulfil-prod%5C%22,%20app_kubernetes_io_name%3D%5C%22cv-repack%5C%22%7D%20%7C%3D%20%60%60%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22P59E27672EF15730F%22%7D,%22editorMode%22:%22builder%22,%22legendFormat%22:%22%22%7D%5D,%22range%22:%7B%22from%22:%22now-6h%22,%22to%22:%22now%22%7D%7D%7D&schemaVersion=1&orgId=1) are the primary observables for Repack.

I think there's some Streamlit stuff somewhere too...

# Testing + Debugging

## 0. Local env setup

Some actions require local environment variables, you can automate setting some of these by creating your own `.env` file:

```
cp sample.env .env
```

Edit the file and change any vars you may end up using, if unsure, just leave unset for now and set once you stumble on a future step.

## 1. Downloading Dataset

For local debugging of (almost) all production data to date, cd into Fulfil.ComputerVision and run this script to download the PLM Repack dataset:

```
docker compose --profile repack_download up --build
```

You'll need [Gcloud SDK](https://cloud.google.com/sdk/docs/install) which the script can also walk you through then double-check your gcloud auth with:

`gcloud auth list`

You'll now have `Fulfil.AlliedVision/data/raw` and `Fulfil.AlliedVision/data/by-id` folders containing raw Repack data and by-id which is conveniently indexed by request_id / primary_key_id for easy debugging.

## 2. Viewing Downloaded Dataset

![COCO annotator setup](annotator.png "COCO annotator setup")

Repack scripting uses the [coco-annotator](https://github.com/jsbroks/coco-annotator/wiki/Getting-Started) to view Repack prod results.  Once running, all you need to do is setup the admin user required by scripts in the "Admin" users section:

![COCO annotator setup](user-setup.png "COCO annotator setup")

After running `Downloading Dataset` above you can aggregate the results into a COCO format for easy bird's eye viewing by running:

```
docker compose --profile repack_load_coco up --build
```

The script will wipe your coco-annotator and create a fresh cv-repack dataset project you can view from the URL generated by the script.

## 3. Viewing request metadata

In coco-annotator, hover over a curious image to view the full image filename, copy/paste it (without the .jpeg extension) into:

`python3 Fulfil.AlliedVision/scripts/repack_inspect_request.py "<My Request ID>"`

to open all sub-images and display metadata.  This runs outside of docker so you can view stuff.

## 4. Replay perception against an arbitrary request ID

After steps 1-3, you can test your code changes against some request ID until you get the desired outcome by running this from Fulfil.AlliedVision:

```
export REQ_ID=677754fa172b1355ef266fe6   # or set in .env
docker compose --profile repack_test up --build
```

where the first and second arguments are "test" action request ID.

### 5. Replay perception against all requests and report output diffs.

```
export REQ_ID=all   # or set in .env
docker compose --profile repack_test up --build
```

This will output debug image files to Fulfil.AlliedVision/data/test/<request id> for further diagnosis.  Since all output labels are rendered too, it's the basis for the next steps...

### 6. Hand-label ground-truths based on machine output

Here we'll use the COCO Annotator viewer from step 2 to do actual ground-truth labeling to determine accuracy.  Start by running step 2 to start fresh, then open COCO Annotator which will be loaded with all the machine outputs.  Scan through and find any images that appear mislabelled to correct them, then export the COCO JSON in the COCO Annotator UI to: `Fulfil.AlliedVision/assets/ground-truths/plm.json` and archive the Fulfil.AlliedVision/data/plm.json machine output file to `Fulfil.AlliedVision/assets/test-outputs/plm.json`.  You may also want to copy both plm.json files to a date-stamped filename for archival or resuming from later.  Now run the summarization script with: 

```
SUMMARIZE_DATASET=plm   # or set in .env
docker compose --profile repack_summarize up --build
```

and you'll get a list of all falsely predicted request IDs as well as the confusion matrix.