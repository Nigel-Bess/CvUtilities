TODO!

# Repack service

The repack service runs only in PLM and responds to FC with requests for whether a bag is empty in a particular Bay.

# CI / CD

Open a PR to auto-build a branch in prod GCR of the same name.  Merging your PR will also rebuild the main branch which is used by PLM.


TODO: Better CD, currently a manual restart is required to pull the latest main image.

# Logging / KPI Graphs

Currently the [Grafana dashboard](https://grafana.fulfil-api.com/d/f8533408-b7a4-4ba7-a087-e95be8d905e3/repack-service?orgId=1&var-ds=P59E27672EF15730F&from=now-3h&to=now) and [raw log search](https://grafana.fulfil-api.com/explore?panes=%7B%22MYp%22:%7B%22datasource%22:%22P59E27672EF15730F%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22%7Bnamespace%3D%5C%22fulfil-prod%5C%22,%20app_kubernetes_io_name%3D%5C%22cv-repack%5C%22%7D%20%7C%3D%20%60%60%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22P59E27672EF15730F%22%7D,%22editorMode%22:%22builder%22,%22legendFormat%22:%22%22%7D%5D,%22range%22:%7B%22from%22:%22now-6h%22,%22to%22:%22now%22%7D%7D%7D&schemaVersion=1&orgId=1) are the primary observables for Repack.

I think there's some Streamlit stuff somewhere too...

# Testing + Debugging

## 1. Downloading Dataset

For local debugging of (almost) all production data to date, cd into this directory and run this script to download the PLM Repack dataset:

```
python3 scripts/repack_download_dataset.py
```

You'll need [Gcloud SDK](https://cloud.google.com/sdk/docs/install) which the script can also walk you through then double-check your gcloud auth with:

`gcloud auth list`

You'll now have `data/raw` and `data/by-id` folders containing raw Repack data and by-id which is conveniently indexed by request_id / primary_key_id for easy debugging.

## 2. Viewing Downloaded Dataset

![COCO annotator setup](annotator.png "COCO annotator setup")

Repack scripting uses the [coco-annotator](https://github.com/jsbroks/coco-annotator/wiki/Getting-Started) to view Repack prod results.  Once running, all you need to do is setup the admin user required by scripts in the "Admin" users section:

![COCO annotator setup](user-setup.png "COCO annotator setup")

After running `Downloading Dataset` above you can aggregate the results into a COCO format for easy bird's eye viewing by running:

```
sudo python3 scripts/coco/repack_generate_coco.py
```

or Run as Admin in Windows since the script must write to your coco-annotator container's shared Docker volume :-( .  The script will wipe your coco-annotator and create a fresh cv-repack dataset project you can view from the URL generated by the script.

## 3. Viewing request metadata

In coco-annotator, hover over a curious image to view the full image filename, copy/paste it (without the .jpeg extension) into:

`python3 scripts/repack_inspect_request.py "<My Request ID>"`

to open all sub-images and display metadata.

## 4. Replay perception against an arbitrary request ID

After steps 1-3, you can test your code changes against some request ID until you get the desired outcome by running this from Fulfil.AlliedVision:

```
docker build .. -f ../Dockerfile -t repackdev;
docker run -v "./data:/home/fulfil/code/Fulfil.ComputerVision/Fulfil.AlliedVision/data" repackdev "Fulfil.AlliedVision/build/test" 677754fa172b1355ef266fe6 "LFB-3.1"
```

where the first and second arguments are request ID and bot generation.

### TODO: 5. Replay perception against all requests and report output diffs.

```
docker build .. -f ../Dockerfile -t repackdev
docker run -v "./data:/home/fulfil/code/Fulfil.ComputerVision/Fulfil.AlliedVision/data" repackdev "Fulfil.AlliedVision/build/test" all
```