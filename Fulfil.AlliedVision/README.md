# Repack service

The repack service runs only in PLM and responds to FC with requests for whether a bag is empty in a particular Bay.

# CI / CD

Open a PR to auto-build a branch in prod GCR of the same name.  Merging your PR will also rebuild the main branch which is used by PLM.


TODO: Better CD, currently a manual restart is required to pull the latest main image.

# Logging / KPI Graphs

Currently the [Grafana dashboard](https://grafana.fulfil-api.com/d/f8533408-b7a4-4ba7-a087-e95be8d905e3/repack-service?orgId=1&var-ds=P59E27672EF15730F&from=now-3h&to=now) and [raw log search](https://grafana.fulfil-api.com/explore?panes=%7B%22MYp%22:%7B%22datasource%22:%22P59E27672EF15730F%22,%22queries%22:%5B%7B%22refId%22:%22A%22,%22expr%22:%22%7Bnamespace%3D%5C%22fulfil-prod%5C%22,%20app_kubernetes_io_name%3D%5C%22cv-repack%5C%22%7D%20%7C%3D%20%60%60%22,%22queryType%22:%22range%22,%22datasource%22:%7B%22type%22:%22loki%22,%22uid%22:%22P59E27672EF15730F%22%7D,%22editorMode%22:%22builder%22,%22legendFormat%22:%22%22%7D%5D,%22range%22:%7B%22from%22:%22now-6h%22,%22to%22:%22now%22%7D%7D%7D&schemaVersion=1&orgId=1) are the primary observables for Repack.

I think there's some Streamlit stuff somewhere too...

# Testing + Debugging

## 1. Downloading Dataset

For local debugging of (almost) all production data to date, cd into this directory and run this script to download the PLM Repack dataset:

```
python3 scripts/repack_download_dataset.py
```

You'll need [Gcloud SDK](https://cloud.google.com/sdk/docs/install) which the script can also walk you through then double-check your gcloud auth with:

`gcloud auth list`

You'll now have `data/raw` and `data/by-id` folders containing raw Repack data and by-id which is conveniently indexed by request_id / primary_key_id for easy debugging.

## 2. Viewing Downloaded Dataset

![COCO annotator setup](annotator.png "COCO annotator setup")

Repack scripting uses the [coco-annotator](https://github.com/jsbroks/coco-annotator/wiki/Getting-Started) to view Repack prod results.  Once running, all you need to do is setup the admin user required by scripts in the "Admin" users section:

![COCO annotator setup](user-setup.png "COCO annotator setup")

After running `Downloading Dataset` above you can aggregate the results into a COCO format for easy bird's eye viewing by running:

```
sudo python3 scripts/coco/repack_generate_coco.py
```

or Run as Admin in Windows since the script must write to your coco-annotator container's shared Docker volume :-( .  The script will wipe your coco-annotator and create a fresh cv-repack dataset project you can view from the URL generated by the script.

## 3. Viewing request metadata

In coco-annotator, hover over a curious image to view the full image filename, copy/paste it (without the .jpeg extension) into:

`python3 scripts/repack_inspect_request.py "<My Request ID>"`

to open all sub-images and display metadata.

## 4. Replay perception against an arbitrary request ID

After steps 1-3, you can test your code changes against some request ID until you get the desired outcome by running this from Fulfil.AlliedVision:

```
docker build .. -f ../Dockerfile -t repackdev;
docker run -v "./data:/home/fulfil/code/Fulfil.ComputerVision/Fulfil.AlliedVision/data" repackdev "Fulfil.AlliedVision/build/test" test 677754fa172b1355ef266fe6
```

where the first and second arguments are "test" action request ID.

## 4b. Replay marker detection against an arbitrary request ID

Run:

```
docker build .. -f ../Dockerfile -t repackdev;
docker run -v "./data:/home/fulfil/code/Fulfil.ComputerVision/Fulfil.AlliedVision/data" repackdev "Fulfil.AlliedVision/build/test" mark 677754fa172b1355ef266fe6
```

where the first and second arguments are "mark" action and request ID.

### 5. Replay perception against all requests and report output diffs.

```
docker build .. -f ../Dockerfile -t repackdev
docker run -v "./data:/home/fulfil/code/Fulfil.ComputerVision/Fulfil.AlliedVision/data" repackdev "Fulfil.AlliedVision/build/test" test all
```

This will output debug image files to data/test/<request id> for further diagnosis.  Since all output labels are rendered too, it's the basis for the next steps...

### 6. Hand-label ground-truths based on machine output

Here we'll use the COCO Annotator viewer from step 2 to do actual ground-truth labeling to determine accuracy.  Start by running step 2 to start fresh, then open COCO Annotator which will be loaded with all the machine outputs.  Scan through and find any images that appear mislabelled to correct them, then export the COCO JSON in the COCO Annotator UI to: `assets/ground-truths/plm.json` and archive the data/plm.json machine output file to `assets/test-outputs/plm.json`.  You may also want to copy both plm.json files to a date-stamped filename for archival or resuming from later.  Now run the summarization script with: `python scripts/coco/repack_summarize.py` and you'll get a list of all falsely predicted request IDs as well as the confusion matrix.